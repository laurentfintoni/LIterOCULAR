<h1 data-src="https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/">We read the <span id="#" class="mention-subject" about="GebruResearchPaper">paper</span> that <span id="#" class="mention-subject" about="Firing">forced <span id="#" class="mention-person" about="TimnitGebru">Timnit Gebru</span> out of <span id="#" class="mention-company" about="Google">Google</span></span>. Here’s what it says.</h1>
<p class="subheader"><span id="#" class="mention-company" about="Google">The company</span>'s <span id="#" class="mention-person" about="TimnitGebru">star ethics researcher</span> highlighted the risks of <span id="#" class="mention-technology" about="NLP">large language models</span>, which are key to <span id="#" class="mention-company" about="Google">Google</span>'s business.</p>
<p class="byline"><span id="#" class="mention-person" about="KarenHao">Karen Hao</span></p>
<p class="publicationDate">December 4, 2020</p>
<p class="publicationSource"><span id="#" class="mention-organization" about="MITTechnologyReview"><span id="#" class="mention-organization" about="MIT">MIT</span> Technology Review</span></p>

<figure>
<img src="#">
<figcaption>Photo Credit</figcaption>
</figure>

<p>On the evening of Wednesday, December 2, <span id="#" class="mention-person" about="TimnitGebru">Timnit Gebru</span>, the co-lead of <span id="#" class="mention-organization" about="GoogleEthicalAI">Google’s ethical AI team</span>, announced via <span id="#" class="mention-technology" about="Twitter">Twitter</span> that <span id="#" class="mention-company" about="Google">the company</span> had <span id="#" class="mention-subject" about="Firing">forced her out</span>.</p> 

<p><span id="#" class="mention-person" about="TimnitGebru">Gebru</span>, a widely respected leader in <span id="#" class="mention-subject" about="AI">AI ethics research</span>, is known for coauthoring a groundbreaking paper that showed <span id="#" class="mention-technology" about="FacialRecognition">facial recognition</span> to be less accurate at identifying <span id="#" class="mention-subject" about="Gender">women</span> and <span id="#" class="mention-subject" about="Identity">people of color</span>, which means <span id="#" class="mention-subject" about="Bias">its use can end up discriminating against them</span>. She also cofounded the <span id="#" class="mention-organization" about="BlackInAI">Black in AI</span> affinity group, and champions diversity in the tech industry. <span id="#" class="mention-organization" about="GoogleEthicalAI">The team</span> she helped build at <span id="#" class="mention-company" about="Google">Google</span> is one of the most diverse in <span id="#" class="mention-technology" about="AI">AI</span> and includes many leading experts in their own right. <span id="#" class="mention-group" about="AICommunity">Peers in the field</span> envied it for producing critical work that often challenged <span id="#" class="mention-subject" about="AI">mainstream AI practices</span>.</p>

<p>A series of tweets, leaked emails, and media articles showed that <span id="#" class="mention-person" about="TimnitGebru">Gebru</span>’s <span id="#" class="mention-subject" about="Firing">exit</span> was the culmination of a conflict over <span id="#" class="mention-subject" about="GebruResearchPaper">another paper</span> she coauthored. <span id="#" class="mention-person" about="JeffDean">Jeff Dean</span>, the head of <span id="#" class="mention-group" about="GoogleAILab">Google AI</span>, told colleagues in an internal email (which he has since put online) that the paper “didn’t meet our bar for publication” and that <span id="#" class="mention-person" about="TimnitGebru">Gebru</span> had said she would resign unless <span id="#" class="mention-company" about="Google">Google</span> met a number of conditions, which it was unwilling to meet. <span id="#" class="mention-person" about="TimnitGebru">Gebru</span> tweeted that she had asked to negotiate “a last date” for her employment after she got back from vacation. She was <span id="#" class="mention-subject" about="Firing">cut off</span> from her corporate email account before her return.</p>

<p>Online, many other <span id="#" class="mention-group" about="AICommunity">leaders in the field of <span id="#" class="mention-subject" about="AI">AI ethics</span></span> are arguing that <span id="#" class="mention-company" about="Google">the company</span> pushed her out because of the inconvenient truths that she was uncovering about a core line of its research—and perhaps its bottom line. More than 1,400 <span id="#" class="mention-company" about="Google">Google</span> staff members and 1,900 other supporters have also signed a letter of protest.</p>

<p>Many details of the exact sequence of events that led up to <span id="#" class="mention-person" about="TimnitGebru">Gebru</span>’s <span id="#" class="mention-subject" about="Firing">departure</span> are not yet clear; both she and <span id="#" class="mention-company" about="Google">Google</span> have declined to comment beyond their posts on <span id="#" class="mention-technology" about="SocialNetworks">social media</span>. But <span id="#" class="mention-organization" about="MITTechnologyReview"><span id="#" class="mention-organization" about="MIT">MIT</span> Technology Review</span> obtained a copy of <span id="#" class="mention-subject" about="GebruResearchPaper">the research paper</span> from one of the coauthors, <span id="#" class="mention-person" about="EmilyMBender">Emily M. Bender</span>, a professor of computational linguistics at the <span id="#" class="mention-organization" about="UniversityOfWashington">University of Washington</span>. Though <span id="#" class="mention-person" about="EmilyMBender">Bender</span> asked us not to publish <span id="#" class="mention-subject" about="GebruResearchPaper">the paper</span> itself because the authors didn’t want such an early draft circulating online, it gives some insight into the questions <span id="#" class="mention-person" about="TimnitGebru">Gebru</span> and her colleagues were raising about <span id="#" class="mention-subject" about="AI">AI</span> that might be causing <span id="#" class="mention-company" about="Google">Google</span> concern.</p>

<p><span id="#" class="mention-subject" about="GebruResearchPaper">“On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?”</span> lays out the risks of <span id="#" class="mention-technology" about="NLP">large language models—AIs</span> trained on <span id="#" class="mention-technology" about="DataSets">staggering amounts of text data</span>. <span id="#" class="mention-technology" about="NLP">These</span> have grown increasingly popular—and increasingly large—in the last three years. <span id="#" class="mention-technology" about="NLP">They</span> are now extraordinarily good, under the right conditions, at producing <span id="#" class="mention-subject" about="IllusionOfMeaning">what looks like convincing, meaningful new text—and sometimes at estimating meaning from language</span>. But, says the introduction to <span id="#" class="mention-subject" about="GebruResearchPaper">the paper</span>, “we ask whether enough thought has been put into the <span id="#" class="mention-subject" about="Bias">potential risks associated with developing them and strategies to mitigate these risks</span>.”</p>

<h2><span id="#" class="mention-subject" about="GebruResearchPaper">The paper</span></h2>

<p><span id="#" class="mention-subject" about="GebruResearchPaper">The paper</span>, which builds on the work of <span id="#" class="mention-group" about="AICommunity">other researchers</span>, presents the history of <span id="#" class="mention-technology" about="NLP">natural-language processing</span>, an overview of four main risks of <span id="#" class="mention-technology" about="NLP">large language models</span>, and suggestions for further research. Since the conflict with <span id="#" class="mention-company" about="Google">Google</span> seems to be over the risks, we’ve focused on summarizing those here. </p>

<h2><span id="#" class="mention-subject" about="EnergyConsumption">Environmental</span> and <span id="#" class="mention-subject" about="AICost">financial costs</span></h2>

<p>Training <span id="#" class="mention-technology" about="AI">large AI models</span> consumes a lot of <span id="#" class="mention-technology" about="CPU">computer processing power</span>, and hence <span id="#" class="mention-subject" about="EnergyConsumption">a lot of electricity</span>. <span id="#" class="mention-person" about="TimnitGebru">Gebru</span> and her coauthors refer to a 2019 paper from <span id="#" class="mention-person" about="EmmaStrubell">Emma Strubell</span> and her collaborators on <span id="#" class="mention-subject" about="EnergyConsumption">the carbon emissions</span> and <span id="#" class="mention-subject" about="AICost">financial costs</span> of <span id="#" class="mention-technology" about="NLP">large language models</span>. It found that their <span id="#" class="mention-subject" about="EnergyConsumption">energy consumption</span> and <span id="#" class="mention-subject" about="EnergyConsumption">carbon footprint</span> have been exploding since 2017, as <span id="#" class="mention-technology" about="NLP">models</span> have been fed <span id="#" class="mention-technology" about="DataSets">more and more data</span>.</p>

<button type="button" class="btn btn-primary" data-bs-toggle="modal" data-bs-target="#carbonModal">
 View chart
</button>

<div class="modal fade" id="carbonModal" tabindex="-1" aria-labelledby="exampleModalLabel" aria-hidden="true">
  <div class="modal-dialog modal-lg">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title text-dark" id="exampleModalLabel">iframes are evil but charts are nice!</h5>
        <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
      </div>
      <div class="modal-body">
       <iframe title="Common carbon footprint benchmarks" aria-label="Bar Chart" id="datawrapper-chart-VQEvf" src="//datawrapper.dwcdn.net/VQEvf/3/" scrolling="no" frameborder="0" style="width:0;min-width:100%;border:none" height="245"></iframe>
      </div>
    </div>
  </div>
</div>

<p><span id="#" class="mention-person" about="EmmaStrubell">Strubell</span>’s study found that training one <span id="#" class="mention-technology" about="NLP">language model</span> with a particular type of <span id="#" class="mention-technology" about="NeuralNetworks">“neural architecture search” (NAS) method</span> would have <span id="#" class="mention-subject" about="EnergyConsumption">produced the equivalent of 626,155 pounds (284 metric tons) of carbon dioxide</span>—about the lifetime output of five average American cars. Training a version of <span id="#" class="mention-company" about="Google">Google</span>’s <span id="#" class="mention-technology" about="NLP">language model</span>, <span id="#" class="mention-technology" about="BERT">BERT</span>, which underpins <span id="#" class="mention-company" about="Google">the company<span id="#" class="mention-company" about="Google">’s <span id="#" class="mention-technology" about="SearchEngines">search engine</span>, <span id="#" class="mention-subject" about="EnergyConsumption">produced 1,438 pounds of CO2 equivalent</span> in <span id="#" class="mention-person" about="EmmaStrubell">Strubell</span>’s estimate—nearly the same as a round-trip flight between <span id="#" class="mention-location" about="NewYorkCity">New York City</span> and <span id="#" class="mention-location" about="SanFrancisco">San Francisco</span>. These numbers should be viewed as minimums, <span id="#" class="mention-subject" about="EnergyConsumption">the cost of training a model one time through</span>. In practice, <span id="#" class="mention-technology" about="NLP">models</span> are trained and retrained many times over during research and development.</p>

<button type="button" class="btn btn-primary" data-bs-toggle="modal" data-bs-target="#costModal">
 View chart
</button>

<div class="modal fade" id="costModal" tabindex="-1" aria-labelledby="exampleModalLabel" aria-hidden="true">
  <div class="modal-dialog modal-lg">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title text-dark" id="exampleModalLabel">iframes are evil but charts are nice!</h5>
        <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
      </div>
      <div class="modal-body">
        <iframe title="The estimated costs of training a model once" aria-label="chart" id="datawrapper-chart-wVVI7" src="https://datawrapper.dwcdn.net/wVVI7/6/" scrolling="no" frameborder="0" style="width:0;min-width:100%;border:none" height="631"></iframe>
      </div>
    </div>
  </div>
</div>

<p><span id="#" class="mention-person" about="TimnitGebru">Gebru</span>’s <span id="#" class="mention-subject" about="GebruResearchPaper">draft paper</span> points out that <span id="#" class="mention-subject" about="EnergyConsumption">the sheer resources</span> required to build and sustain <span id="#" class="mention-technology" about="AI">such large AI models</span> means <span id="#" class="mention-subject" about="Bias">they tend to benefit wealthy organizations, while climate change hits marginalized communities hardest</span>. “It is past time for researchers to <span id="#" class="mention-subject" about="EnergyConsumption">prioritize energy efficiency</span> and <span id="#" class="mention-subject" about="AICost">cost</span> to reduce negative environmental impact and inequitable access to resources,” they write.</p>

<h2><span id="#" class="mention-technology" about="DataSets">Massive data</span>, <span id="#" class="mention-technology" about="NLP">inscrutable models</span></h2>

<p><span id="#" class="mention-technology" about="NLP">Large language models</span> are also trained on <span id="#" class="mention-technology" about="DataSets">exponentially increasing amounts of text</span>. This means researchers have sought to collect <span id="#" class="mention-technology" about="DataSets">all the data</span> they can from <span id="#" class="mention-technology" about="Internet">the internet</span>, so there's a risk that <span id="#" class="mention-subject" about="Racism">racist</span>, <span id="#" class="mention-subject" about="Sexism">sexist</span>, and otherwise abusive language ends up in <span id="#" class="mention-technology" about="DataSets">the training data</span>.</p>

<p><span id="#" class="mention-technology" about="NLP">An AI model</span> taught to view <span id="#" class="mention-subject" about="Racism">racist language</span> as normal is obviously bad. The researchers, though, point out a couple of more subtle problems. One is that shifts in language play an important role in social change; the <span id="#" class="mention-subject" about="MeToo">MeToo</span> and <span id="#" class="mention-organization" about="BLM">Black Lives Matter</span> movements, for example, have tried to establish a new anti-sexist and anti-racist vocabulary. <span id="#" class="mention-technology" about="NLP">An AI model</span> trained on vast swaths of <span id="#" class="mention-technology" about="Internet">the internet</span> <span id="#" class="mention-subject" about="Bias">won’t be attuned to the nuances of this vocabulary and won’t produce or interpret language in line with these new cultural norms</span>.</p>

<p><span id="#" class="mention-technology" about="NLP">It</span> will also <span id="#" class="mention-subject" about="Bias">fail to capture the language and the norms of countries and peoples that have less access to <span id="#" class="mention-technology" about="Internet">the internet</span> and thus a smaller linguistic footprint online</span>. The result is that <span id="#" class="mention-technology" about="NLP">AI-generated language</span> <span id="#" class="mention-subject" about="Bias">will be homogenized, reflecting the practices of the richest countries and communities</span>.</p>

<p>Moreover, because <span id="#" class="mention-technology" about="DataSets">the training data sets</span> are so large, it’s hard to audit them to check for these <span id="#" class="mention-subject" about="Bias">embedded biases</span>. “A methodology that relies on <span id="#" class="mention-technology" about="DataSets">datasets too large</span> to document is therefore inherently risky,” the researchers conclude. “While documentation allows for potential accountability, [...] <span id="#" class="mention-technology" about="DataSets">undocumented training data</span> <span id="#" class="mention-subject" about="Bias">perpetuates harm without recourse</span>.”</p>

<h2><span id="#" class="mention-subject" about="MisdirectedResearch">Research opportunity costs</span></h2>

<p>The researchers summarize the third challenge as <span id="#" class="mention-subject" about="MisdirectedResearch">the risk of “misdirected research effort.”</span> Though <span id="#" class="mention-group" about="AICommunity">most AI researchers</span> acknowledge that <span id="#" class="mention-technology" about="NLP">large language models</span> don’t actually understand language and are merely excellent at manipulating it, <span id="#" class="mention-subject" about="TechCompanies">Big Tech</span> can make money from <span id="#" class="mention-technology" about="NLP">models that manipulate language more accurately</span>, so it keeps investing in them. “This research effort brings with it an opportunity cost,” <span id="#" class="mention-person" about="TimnitGebru">Gebru</span> and her colleagues write. Not as much effort goes into working on <span id="#" class="mention-technology" about="NLP">AI models</span> that <span id="#" class="mention-subject" about="MisdirectedResearch">might achieve understanding, or that achieve good results with smaller, <span id="#" class="mention-technology" about="DataSets">more carefully curated data sets</span> (and thus also use less energy)</span>.</p>

<h2><span id="#" class="mention-subject" about="IllusionOfMeaning">Illusions of meaning</span></h2>

<p>The final problem with <span id="#" class="mention-technology" about="NLP">large language models</span>, the researchers say, is that <span id="#" class="mention-subject" about="IllusionOfMeaning">because they’re so good at mimicking real human language, it’s easy to use them to fool people</span>. There have been a few high-profile cases, such as <span id="#" class="mention-subject" about="IllusionOfMeaning">the college student who churned out <span id="#" class="mention-technology" about="AI">AI-generated self-help and productivity advice</span> on a blog</span>, which went viral.</p>

<p>The dangers are obvious: <span id="#" class="mention-technology" about="AI">AI models</span> could be used to <span id="#" class="mention-subject" about="IllusionOfMeaning">generate misinformation about an election or the covid-19 pandemic</span>, for instance. They can also <span id="#" class="mention-subject" about="IllusionOfMeaning">go wrong inadvertently when used for machine translation</span>. The researchers bring up an example: In 2017, <span id="#" class="mention-company" about="Facebook">Facebook</span> <span id="#" class="mention-subject" about="IllusionOfMeaning">mistranslated a Palestinian man’s post, which said “good morning” in Arabic, as “attack them” in Hebrew</span>, leading to his arrest.</p>

<h2>Why it matters</h2>

<p><span id="#" class="mention-person" about="TimnitGebru">Gebru</span> and <span id="#" class="mention-person" about="EmilyMBender">Bender</span>’s paper has six coauthors, four of whom are <span id="#" class="mention-company" about="Google">Google</span> researchers. <span id="#" class="mention-person" about="EmilyMBender">Bender</span> asked to avoid disclosing their names for fear of repercussions. (<span id="#" class="mention-person" about="EmilyMBender">Bender</span>, by contrast, is a tenured professor: “I think this is underscoring the value of academic freedom,” she says.)</p>

<p><span id="#" class="mention-subject" about="GebruResearchPaper">The paper’s goal</span>, <span id="#" class="mention-person" about="EmilyMBender">Bender</span> says, was to take stock of the landscape of current research in <span id="#" class="mention-technology" about="NLP">natural-language processing</span>. “We are working at a scale where <span id="#" class="mention-group" about="AICommunity">the people building the things</span> can’t actually get their arms around <span id="#" class="mention-technology" about="DataSets">the data</span>,” she said. “And because the upsides are so obvious, it’s particularly important to step back and ask ourselves, what are <span id="#" class="mention-subject" about="Bias">the possible downsides? … How do we get the benefits of this while mitigating the risk?</span>”</p>

<p>In his internal email, <span id="#" class="mention-person" about="JeffDean">Dean</span>, the <span id="#" class="mention-group" about="GoogleAILab">Google AI</span> head, said one reason <span id="#" class="mention-subject" about="GebruResearchPaper">the paper</span> “didn’t meet our bar” was that it “ignored too much relevant research.” Specifically, <span id="#" class="mention-person" about="JeffDean">he</span> said it didn’t mention more recent work on how to make <span id="#" class="mention-technology" about="NLP">large language models</span> <span id="#" class="mention-subject" about="EnergyConsumption">more energy efficient</span> and <span id="#" class="mention-subject" about="Bias">mitigate problems of bias</span>.</p>

<p>However, the six collaborators drew on a wide breadth of scholarship. <span id="#" class="mention-subject" about="GebruResearchPaper">The paper</span>’s citation list, with 128 references, is notably long. “It’s the sort of work that no individual or even pair of authors can pull off,” <span id="#" class="mention-person" about="EmilyMBender">Bender</span> said. “It really required this collaboration.”</p>

<p>The version of <span id="#" class="mention-subject" about="GebruResearchPaper">the paper</span> we saw does also nod to several research efforts on <span id="#" class="mention-subject" about="EnergyConsumption">reducing the size and computational costs</span> of <span id="#" class="mention-technology" about="NLP">large language models</span>, and on <span id="#" class="mention-subject" about="Bias">measuring the embedded bias of models</span>. It argues, however, that these efforts have not been enough. “I’m very open to seeing what other references we ought to be including,” <span id="#" class="mention-person" about="EmilyMBender">Bender</span> said.</p>

<p><span id="#" class="mention-person" about="NicolasLeRoux">Nicolas Le Roux</span>, a <span id="#" class="mention-group" about="GoogleAILab">Google AI</span> researcher in the <span id="#" class="mention-location" about="Montreal">Montreal</span> office, later noted on <span id="#" class="mention-technology" about="Twitter">Twitter</span> that the reasoning in <span id="#" class="mention-person" about="JeffDean">Dean</span>’s email was unusual. “My submissions were always checked for disclosure of sensitive material, never for the quality of the literature review,” he said.</p>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Now might be a good time to remind everyone that the easiest way to discriminate is to make stringent rules, then to decide when and for whom to enforce them.<br>My submissions were always checked for disclosure of sensitive material, never for the quality of the literature review.</p>&mdash; Nicolas Le Roux (@le_roux_nicolas) <a href="https://twitter.com/le_roux_nicolas/status/1334601960972906496?ref_src=twsrc%5Etfw">December 3, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p><span id="#" class="mention-person" about="JeffDean">Dean</span>’s email also says that <span id="#" class="mention-person" about="TimnitGebru">Gebru</span> and her colleagues gave <span id="#" class="mention-group" about="GoogleAILab">Google AI</span> only a day for an internal review of <span id="#" class="mention-subject" about="GebruResearchPaper">the paper</span> before they submitted it to a conference for publication. <span id="#" class="mention-person" about="JeffDean">He</span> wrote that “our aim is to rival peer-reviewed journals in terms of the rigor and thoughtfulness in how we review research before publication.”</p>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I understand the concern over Timnit’s resignation from Google. She’s done a great deal to move the field forward with her research. I wanted to share the email I sent to Google Research and some thoughts on our research process.<a href="https://t.co/djUGdYwNMb">https://t.co/djUGdYwNMb</a></p>&mdash; Jeff Dean (@🏡) (@JeffDean) <a href="https://twitter.com/JeffDean/status/1334953632719011840?ref_src=twsrc%5Etfw">December 4, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p><span id="#" class="mention-person" about="EmilyMBender">Bender</span> noted that even so, the conference would still put the paper through a substantial review process: “Scholarship is always a conversation and always a work in progress,” she said. </p>

<p>Others, including <span id="#" class="mention-person" about="WilliamFitzgerald">William Fitzgerald</span>, a former <span id="#" class="mention-company" about="Google">Google</span> PR manager, have further cast doubt on <span id="#" class="mention-person" about="JeffDean">Dean</span>’s claim.</p>

<p><span id="#" class="mention-company" about="Google">Google</span> pioneered much of the foundational research that has since led to the recent explosion in <span id="#" class="mention-technology" about="NLP">large language models</span>. <span id="#" class="mention-group" about="GoogleAILab">Google AI</span> was the first to invent <span id="#" class="mention-technology" about="NLP">the Transformer language model</span> in 2017 that serves as the basis for the company’s later model <span id="#" class="mention-technology" about="BERT">BERT</span>, and <span id="#" class="mention-company" about="OpenAI">OpenAI</span>’s <span id="#" class="mention-technology" about="GPT">GPT-2 and GPT-3</span>. <span id="#" class="mention-technology" about="BERT">BERT</span>, as noted above, now also powers <span id="#" class="mention-technology" about="SearchEngines">Google search</span>, <span id="#" class="mention-company" about="Google">the company</span>’s cash cow.</p>

<p><span id="#" class="mention-person" about="EmilyMBender">Bender</span> worries that <span id="#" class="mention-company" about="Google">Google</span>’s actions could create “a chilling effect” on future <span id="#" class="mention-subject" about="AI">AI ethics research</span>. Many of <span id="#" class="mention-group" about="AICommunity">the top experts</span> in <span id="#" class="mention-subject" about="AI">AI ethics</span> work at <span id="#" class="mention-subject" about="TechCompanies">large tech companies</span> because that is where the money is. “That has been beneficial in many ways,” <span id="#" class="mention-person" about="EmilyMBender">she</span> says. “But we end up with an ecosystem that maybe <span id="#" class="mention-subject" about="Bias">has incentives that are not the very best ones for the progress of science for the world</span>.”</p>

<p><em><b>Update (Dec 7):</b> Additional details have been added to clarify the environmental costs of large language models.</em></p>