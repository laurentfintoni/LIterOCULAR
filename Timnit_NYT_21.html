<figure class="timnit-nyt-f">
    <img src="/img/nyt1.jpg" class="img-fluid" alt="image of Timnit Gebru" title="Portrait of Timnit Gebru">
    <figcaption class="timnit-nyt-c">Photo credit</figcaption>
    </figure>    

<h1 class="timnit-nyt-t" data-src="https://www.nytimes.com/2021/03/15/technology/artificial-intelligence-google-bias.html"><span class="timnit-nyt-green">Who</span> Is Making Sure the <span class="timnit-nyt-green"><span id="#" class="mention-technology" about="AI">A.I. Machines</span></span> Aren’t <span class="timnit-nyt-green"><span id="#" class="mention-subject" about="Racism">Racist</span></span>?</h1>
<p class="subheader timnit-nyt-s">When <span id="#" class="mention-company" about="Google">Google</span> forced out two well-known <span class="timnit-nyt-green"><span id="#" class="mention-subject" about="AI">artificial intelligence</span> experts</span>, a long-simmering research controversy burst into the open.</p>
<p class="byline timnit-nyt-b"><span id="#" class="mention-person" about="CadeMetz">Cade Metz</span> | 
<span class="date-emoji">March 15, 2021</span> | <span id="#" class="mention-organization pub-emoji" about="TheNewYorkTimes">The New York Times</span></p>

<p class="dropcap-nyt">Hundreds of people gathered for the first lecture at what had become <span id="#" class="mention-event" about="AIConference">the world’s most important conference on <span id="#" class="mention-subject" about="AI">artificial intelligence</span></span> — row after row of faces. Some were <span id="#" class="mention-subject" about="Identity">East Asian</span>, a few were <span id="#" class="mention-subject" about="Identity">Indian</span>, and a few were <span id="#" class="mention-subject" about="Gender">women</span>. But the vast majority were <span id="#" class="mention-subject" about="Identity">white <span id="#" class="mention-subject" about="Gender">men</span></span>. More than 5,500 people attended <span id="#" class="mention-event" about="AIConference">the meeting</span>, five years ago in <span id="#" class="mention-location" about="Barcelona">Barcelona, Spain</span>.</p>

<p><span id="#" class="mention-person" about="TimnitGebru">Timnit Gebru</span>, then a graduate student at <span id="#" class="mention-organization" about="Stanford">Stanford University</span>, remembers counting only six <span id="#" class="mention-subject" about="Identity">Black people</span> other than herself, all of whom she knew, all of whom were <span id="#" class="mention-subject" about="Gender">men</span>.</p>

<p><span id="#" class="mention-subject" about="Identity">The homogeneous crowd</span> crystallized for <span id="#" class="mention-person" about="TimnitGebru">her</span> a glaring issue. The <span id="#" class="mention-subject" about="TechCompanies">big thinkers of tech</span> say <span id="#" class="mention-subject" about="AI">A.I. is the future</span>. It will underpin everything from <span id="#" class="mention-technology" about="SearchEngines">search engines</span> and <span id="#" class="mention-technology" about="Email">email</span> to the <span id="#" class="mention-technology" about="AISoftware">software that drives our cars, directs the <span id="#" class="mention-organization" about="LawEnforcement">policing</span> of our streets and helps create our vaccines</span>.</p>

<p>But <span id="#" class="mention-technology" about="AI">it</span> is being built in a way that replicates <span id="#" class="mention-subject" about="Bias">the biases of the almost entirely <span id="#" class="mention-subject" about="Gender">male</span>, predominantly <span id="#" class="mention-subject" about="Identity">white</span> work force making it</span>.</p>

<p>In the nearly 10 years I’ve written about <span id="#" class="mention-subject" about="AI">artificial intelligence</span>, two things have remained a constant: <span id="#" class="mention-technology" about="AI">The technology</span> relentlessly improves in fits and sudden, great leaps forward. And <span id="#" class="mention-subject" about="Bias">bias</span> is a thread that subtly weaves through that work in a way that <span id="#" class="mention-company" about="TechCompanies">tech companies</span> are reluctant to acknowledge.</p>

<p>On her first night home in <span id="#" class="mention-location" about="MenloPark">Menlo Park, Calif.</span>, after the <span id="#" class="mention-event" about="AIConference"><span id="#" class="mention-location" about="Barcelona">Barcelona</span> conference</span>, sitting cross-​legged on the couch with her laptop, <span id="#" class="mention-person" about="TimnitGebru">Dr. Gebru</span> described the <span id="#" class="mention-group" about="AICommunity">A.I. work force</span> conundrum in a <span id="#" class="mention-company" about="Facebook">Facebook</span> post.</p>

<p>“I’m not worried about machines taking over the world. I’m worried about groupthink, insularity and arrogance in the <span id="#" class="mention-group" about="AICommunity">A.I. community</span> — especially with the current hype and demand for people in the field,” she wrote. “<span id="#" class="mention-group" about="AICommunity">The people</span> creating <span id="#" class="mention-technology" about="AI">the technology</span> are a big part of the system. If many are actively excluded from its creation, this <span id="#" class="mention-technology" about="AI">technology</span> <span id="#" class="mention-subject" about="Bias">will benefit a few while harming a great many</span>.”</p>

<p>The <span id="#" class="mention-group" about="AICommunity">A.I. community</span> buzzed about the mini-manifesto. Soon after, <span id="#" class="mention-person" about="TimnitGebru">Dr. Gebru</span> helped create a new organization, <span id="#" class="mention-organization" about="BlackInAI">Black in A.I.</span> After finishing her Ph.D., she was hired by <span id="#" class="mention-company" about="Google">Google</span>.</p>

<p>She teamed with <span id="#" class="mention-person" about="MargaretMitchell">Margaret Mitchell</span>, who was building a <span id="#" class="mention-group" about="GoogleEthicalAI">group inside <span id="#" class="mention-company" about="Google">Google</span> dedicated to <span id="#" class="mention-subject" about="AI">“ethical A.I.”</span></span> <span id="#" class="mention-person" about="MargaretMitchell">Dr. Mitchell</span> had previously worked in the research lab at <span id="#" class="mention-company" about="Microsoft">Microsoft</span>. She had grabbed attention when she told <span id="#" class="mention-organization" about="Bloomberg">Bloomberg News</span> in 2016 that <span id="#" class="mention-subject" about="AI">A.I.</span> suffered from a <span id="#" class="mention-subject" about="Gender">“sea of dudes”</span> problem. She estimated that she had worked with hundreds of <span id="#" class="mention-subject" about="Gender">men</span> over the previous five years and about 10 <span id="#" class="mention-subject" about="Bias">women</span>.</p>

<p>Their work was hailed as groundbreaking. The nascent <span id="#" class="mention-group" about="AICommunity">A.I. industry</span>, it had become clear, needed minders and people with different perspectives.</p>

<p>About six years ago, <span id="#" class="mention-subject" about="AI">A.I.</span> in a <span id="#" class="mention-company" about="Google">Google</span> <span id="#" class="mention-technology" about="GooglePhotos">online photo service</span> organized photos of <span id="#" class="mention-subject" about="Identity">Black people</span> into a folder called “gorillas.” Four years ago, a researcher at a <span id="#" class="mention-location" about="NewYorkCity">New York</span> start-up noticed that the <span id="#" class="mention-technology" about="AI">A.I. system</span> she was working on was egregiously <span id="#" class="mention-subject" about="Bias">biased against <span id="#" class="mention-subject" about="Identity">Black people</span></span>. Not long after, a <span id="#" class="mention-subject" about="Identity">Black</span> researcher in <span id="#" class="mention-location" about="Boston">Boston</span> discovered that an <span id="#" class="mention-technology" about="AI">A.I. system</span> couldn’t identify her face — until she put on a <span id="#" class="mention-subject" about="Identity">white</span> mask.</p>

<p>In 2018, when I told <span id="#" class="mention-company" about="Google">Google</span>’s public relations staff that I was working on a book about <span id="#" class="mention-subject" about="AI">artificial intelligence</span>, it arranged a long talk with <span id="#" class="mention-person" about="MargaretMitchell">Dr. Mitchell</span> to discuss her work. As she described how she built the company’s <span id="#" class="mention-group" about="GoogleEthicalAI">Ethical A.I. team</span> — and brought <span id="#" class="mention-person" about="TimnitGebru">Dr. Gebru</span> into the fold — it was refreshing to hear from someone so closely focused on the <span id="#" class="mention-subject" about="Bias">bias problem</span>.</p>

<p>But nearly three years later, <span id="#" class="mention-person" about="TimnitGebru">Dr. Gebru</span> was pushed out of the company without a clear explanation. She said she had been <span id="#" class="mention-subject" about="Firing">fired</span> after criticizing <span id="#" class="mention-company" about="Google">Google</span>’s approach to minority hiring and, with a <span id="#" class="mention-subject" about="GebruResearchPaper">research paper</span>, highlighting the <span id="#" class="mention-subject" about="Bias">harmful biases in the <span id="#" class="mention-technology" about="AI">A.I. systems</span></span> that underpin <span id="#" class="mention-company" about="Google">Google</span>’s <span id="#" class="mention-technology" about="SearchEngines">search engine</span> and other services.</p>

<p>“Your life starts getting worse when you start advocating for underrepresented people,” <span id="#" class="mention-person" about="TimnitGebru">Dr. Gebru</span> said in an email before her firing. “You start making the other leaders upset.”</p>

<p>As <span id="#" class="mention-person" about="MargaretMitchell">Dr. Mitchell</span> defended <span id="#" class="mention-person" about="TimnitGebru">Dr. Gebru</span>, the company <span id="#" class="mention-subject" about="Firing">removed her</span>, too. She had searched through her own <span id="#" class="mention-company" about="Google">Google</span> email account for material that would support their position and forwarded emails to another account, which somehow got her into trouble. <span id="#" class="mention-company" about="Google">Google</span> declined to comment for this article.</p>

<p><span id="#" class="mention-subject" about="Firing">Their departure</span> became a point of contention for <span id="#" class="mention-subject" about="AICommunity">A.I. researchers</span> and other tech workers. Some saw a giant company no longer willing to listen, too eager to get <span id="#" class="mention-subject" about="Technology">technology</span> out the door without considering its implications. I saw an old problem — part technological and part sociological — finally breaking into the open.</p>

<figure>
<img src="/img/nyt2.jpg" class="img-fluid" alt="image of Google building" title="Google building">
<figcaption><span id="#" class="mention-technology" about="AI">Artificial intelligence technology</span> will eventually find its way into almost everything <span id="#" class="mention-company" about="Google">Google</span> does. Credit...<span id="#" class="mention-person" about="CodyOLoughlin">Cody O'Loughlin</span> for <span id="#" class="mention-organization" about="TheNewYorkTimes">The New York Times</span></figcaption>
</figure>

<p>It should have been a wake-up call.</p>

<p>In June 2015, a friend sent <span id="#" class="mention-person" about="JackyAlcine">Jacky Alciné</span>, a 22-year-old software engineer living in <span id="#" class="mention-location" about="Brooklyn">Brooklyn</span>, an internet link for snapshots the friend had posted to the new <span id="#" class="mention-technology" about="GooglePhotos">Google Photos service</span>. <span id="#" class="mention-technology" about="GooglePhotos">Google Photos</span> could <span id="#" class="mention-technology" about="FacialRecognition">analyze snapshots and automatically sort them into digital folders based on what was pictured</span>. One folder might be “dogs,” another “birthday party.”</p>

<p>When <span id="#" class="mention-person" about="JackyAlcine">Mr. Alciné</span> clicked on the link, he noticed one of the folders was labeled “gorillas.” That made no sense to him, so he opened the folder. He found more than 80 photos he had taken nearly a year earlier of a friend during a concert in nearby <span id="#" class="mention-location" about="Brooklyn">Prospect Park</span>. That friend was <span id="#" class="mention-subject" about="Identity">Black</span>.</p>

<p>He might have let it go if <span id="#" class="mention-company" about="Google">Google</span> had mistakenly tagged just one photo. But 80? He posted a screenshot on <span id="#" class="mention-technology" about="Twitter">Twitter</span>. “<span id="#" class="mention-technology" about="GooglePhotos">Google Photos</span>, y’all,” messed up, he wrote, using much saltier language. “My friend is not a gorilla.”</p>

<p>Like <span id="#" class="mention-technology" about="FacialRecognition">facial recognition services</span>, <span id="#" class="mention-technology" about="DigitalAssistant">talking digital assistants</span> and conversational <span id="#" class="mention-technology" about="Chatbots">“chatbots,”</span> <span id="#" class="mention-technology" about="GooglePhotos">Google Photos</span> relied on an <span id="#" class="mention-technology" about="AI">A.I. system</span> that learned its skills by analyzing <span id="#" class="mention-technology" about="DataSets">enormous amounts of digital data</span>.</p>

<p>Called a <span id="#" class="mention-technology" about="NeuralNetworks">“neural network,”</span> this mathematical system could learn tasks that engineers could never code into a machine on their own. By analyzing thousands of photos of gorillas, it could learn to recognize a gorilla. It was also capable of egregious mistakes. The onus was on engineers to choose <span id="#" class="mention-technology" about="DataSets">the right data</span> when training these mathematical systems. (In this case, the easiest fix was to eliminate “gorilla” as a photo category.)</p>

<p>As a software engineer, <span id="#" class="mention-person" about="JackyAlcine">Mr. Alciné</span> understood the problem. He compared it to making lasagna. “If you mess up the lasagna ingredients early, the whole thing is ruined,” he said. “It is the same thing with <span id="#" class="mention-subject" about="AI">A.I.</span> <span id="#" class="mention-subject" about="Bias">You have to be very intentional about what you put into it</span>. Otherwise, it is very difficult to undo.”</p>

<h2 class="nyt-header pron">The Porn Problem</h2>

<p class="after-box">In 2017, <span id="#" class="mention-person" about="DeborahRaji">Deborah Raji</span>, a 21-​year-​old <span id="#" class="mention-subject" about="Identity">Black</span> <span id="#" class="mention-subject" about="Gender">woman</span> from <span id="#" class="mention-location" about="Ottawa">Ottawa</span>, sat at a desk inside the <span id="#" class="mention-location" about="NewYorkCity">New York</span> offices of <span id="#" class="mention-company" about="Clarifai">Clarifai</span>, the start-up where she was working. The company built <span id="#" class="mention-technology" about="FacialRecognition">technology that could automatically recognize objects in digital images</span> and planned to sell it to businesses, <span id="#" class="mention-organization" about="LawEnforcement">police departments</span> and <span id="#" class="mention-group" about="USGovernment">government agencies</span>.</p>

<p>She stared at a screen filled with faces — images the company used to train its <span id="#" class="mention-technology" about="FacialRecognition">facial recognition software</span>.</p>

<p>As she scrolled through page after page of these faces, she realized that most — more than 80 percent — were of <span id="#" class="mention-subject" about="Identity">white people</span>. More than 70 percent of those <span id="#" class="mention-subject" about="Identity">white people</span> were <span id="#" class="mention-subject" about="Gender">male</span>. When <span id="#" class="mention-company" about="Clarifai">Clarifai</span> <span id="#" class="mention-technology" about="DataSets">trained its system on this data</span>, it might do a decent job of recognizing <span id="#" class="mention-subject" about="Identity">white people</span>, <span id="#" class="mention-person" about="DeborahRaji">Ms. Raji</span> thought, but it would fail miserably with <span id="#" class="mention-subject" about="Identity">people of color</span>, and probably <span id="#" class="mention-subject" about="Gender">women</span>, too.</p>

<figure>
<img src="/LiterOCULAR/img/nyt3.jpg" class="img-fluid" alt="image of Deborah Raji" title="Deborah Raji portrait">
<figcaption><span id="#" class="mention-person" about="DeborahRaji">Deborah Raji</span> realized that a company’s technology wasn’t getting the input it needed to properly recognize <span id="#" class="mention-subject" about="Identity">people of color</span>. Credit...<span id="#" class="mention-person" about="JaimeHogge">Jaime Hogge</span> for <span id="#" class="mention-organization" about="TheNewYorkTimes">The New York Times</span></figcaption>
</figure>

<p><span id="#" class="mention-company" about="Clarifai">Clarifai</span> was also building a <span id="#" class="mention-technology" about="ContentModerationSystem">“content moderation system,”</span> a tool that could automatically identify and remove pornography from images people posted to <span id="#" class="mention-technology" about="SocialNetworks">social networks</span>. The company trained this system on <span id="#" class="mention-technology" about="DataSets">two sets of data: thousands of photos pulled from online pornography sites, and thousands of G‑rated images bought from stock photo services</span>.</p>

<p><span id="#" class="mention-technology" about="ContentModerationSystem">The system</span> was supposed to learn the difference between the pornographic and the anodyne. The problem was that the G‑rated images were dominated by <span id="#" class="mention-subject" about="Identity">white people</span>, and the pornography was not. <span id="#" class="mention-subject" about="Bias"><span id="#" class="mention-technology" about="ContentModerationSystem">The system</span> was learning to identify <span id="#" class="mention-subject" about="Identity">Black people</span> as pornographic</span>.</p>

<p>“<span id="#" class="mention-technology" about="DataSets">The data we use to train <span id="#" class="mention-technology" about="AI">these systems</span></span> matters,” <span id="#" class="mention-person" about="DeborahRaji">Ms. Raji</span> said. “<span id="#" class="mention-subject" about="Bias">We can’t just blindly pick our sources</span>.”</p>

<p>This was obvious to her, but to the rest of the <span id="#" class="mention-company" about="Clarifai">company</span> it was not. Because the people choosing <span id="#" class="mention-technology" about="DataSets">the training data</span> were mostly <span id="#" class="mention-subject" about="Identity">white men</span>, they didn’t realize <span id="#" class="mention-subject" about="Bias"><span id="#" class="mention-technology" about="DataSets">their data</span> was biased</span>.</p>

<p>“The issue of <span id="#" class="mention-subject" about="Bias">bias in <span id="#" class="mention-technology" about="FacialRecognition">facial recognition technologies</span> is an evolving and important topic</span>,” <span id="#" class="mention-company" about="Clarifai">Clarifai</span>’s chief executive, <span id="#" class="mention-person" about="MattZeiler">Matt Zeiler</span>, said in a statement. <span id="#" class="mention-subject" about="Bias">Measuring bias, he said, “is an important step.”</span></p>

<h2 class="nyt-header mask">‘<span id="#" class="mention-subject" about="Identity">Black Skin, White Masks</span>’</h2>

<p class="after-box">Before joining <span id="#" class="mention-company" about="Google">Google</span>, <span id="#" class="mention-person" about="TimnitGebru">Dr. Gebru</span> collaborated on a study with a young computer scientist, <span id="#" class="mention-person" about="JoyBuolamwini">Joy Buolamwini</span>. A graduate student at the <span id="#" class="mention-organization" about="MIT">Massachusetts Institute of Technology</span>, <span id="#" class="mention-person" about="JoyBuolamwini">Ms. Buolamwini</span>, who is <span id="#" class="mention-subject" about="Identity">Black</span>, came from a family of academics. Her grandfather specialized in medicinal chemistry, and so did her father.</p>

<p>She gravitated toward <span id="#" class="mention-technology" about="FacialRecognition">facial recognition technology</span>. <span id="#" class="mention-group" about="AICommunity">Other researchers</span> believed it was reaching maturity, but when she used it, she knew it wasn’t.</p>

<p>In October 2016, a friend invited her for a night out in <span id="#" class="mention-location" about="Boston">Boston</span> with several other women. “We’ll do masks,” the friend said. Her friend meant skin care masks at a spa, but <span id="#" class="mention-person" about="JoyBuolamwini">Ms. Buolamwini</span> assumed Halloween masks. So she carried a white plastic Halloween mask to her office that morning.</p>

<p>It was still sitting on her desk a few days later as she struggled to finish a project for one of her classes. She was trying to get a <span id="#" class="mention-technology" about="FacialRecognition">detection system</span> to track her face. No matter what she did, she couldn’t quite get it to work.</p>

<p>In her frustration, <span id="#" class="mention-subject" about="Bias">she picked up the white mask from her desk and pulled it over her head. Before it was all the way on, the system recognized her face — or, at least, it recognized the mask</span>.</p>

<p>“<span id="#" class="mention-subject" about="Identity">Black Skin, White Masks</span>,” <span id="#" class="mention-person" about="JoyBuolamwini">she</span> said in an interview, nodding to the 1952 <span id="#" class="mention-subject" about="Racism">critique of historical racism</span> from the psychiatrist <span id="#" class="mention-person" about="FrantzFanon">Frantz Fanon</span>. “The metaphor becomes the truth. <span id="#" class="mention-subject" about="Racism">You have to fit a norm, and that norm is not you</span>.”</p>

<p><span id="#" class="mention-person" about="JoyBuolamwini">Ms. Buolamwini</span> started exploring <span id="#" class="mention-technology" about="FacialRecognition">commercial services designed to analyze faces and identify characteristics like age and sex</span>, including tools from <span id="#" class="mention-company" about="Microsoft">Microsoft</span> and <span id="#" class="mention-company" about="IBM">IBM</span>.</p>

<p><span id="#" class="mention-person" about="JoyBuolamwini">She</span> found that when the services read photos of lighter-​skinned <span id="#" class="mention-subject" about="Gender">men</span>, they misidentified <span id="#" class="mention-subject" about="Gender">sex</span> about 1 percent of the time. <span id="#" class="mention-subject" about="Bias">But the darker the skin in the photo, the larger the error rate</span>. It rose particularly high with images of <span id="#" class="mention-subject" about="Gender">women</span> with dark skin. <span id="#" class="mention-company" about="Microsoft">Microsoft</span>’s error rate was about 21 percent. <span id="#" class="mention-company" about="IBM">IBM</span>’s was 35.</p>

<p>Published in the winter of 2018, the study drove a backlash against <span id="#" class="mention-technology" about="FacialRecognition">facial recognition technology</span> and, particularly, its use in <span id="#" class="mention-organization" about="LawEnforcement">law enforcement</span>. <span id="#" class="mention-company" about="Microsoft">Microsoft</span>’s chief legal officer said the company had turned down sales to <span id="#" class="mention-organization" about="LawEnforcement">law enforcement</span> when there was concern the technology could unreasonably infringe on people’s rights, and he made a public call for <span id="#" class="mention-organization" about="USGovernment">government</span> regulation.</p>

<p>Twelve months later, <span id="#" class="mention-company" about="Microsoft">Microsoft</span> backed a bill in <span id="#" class="mention-location" about="WashingtonState">Washington State</span> that would require notices to be posted in public places using <span id="#" class="mention-technology" about="FacialRecognition">facial recognition</span> and ensure that <span id="#" class="mention-organization" about="USGovernment">government agencies</span> obtained a court order when looking for specific people. The bill passed, and it takes effect later this year. <span id="#" class="mention-company" about="Microsoft">The company</span>, which did not respond to a request for comment for this article, did not back other legislation that would have provided stronger protections.</p>

<p><span id="#" class="mention-person" about="JoyBuolamwini">Ms. Buolamwini</span> began to collaborate with <span id="#" class="mention-person" about="DeborahRaji">Ms. Raji</span>, who moved to <span id="#" class="mention-organization" about="MIT">M.I.T.</span> They started testing <span id="#" class="mention-technology" about="FacialRecognition">facial recognition technology</span> from a third American <span id="#" class="mention-subject" about="TechCompanies">tech giant</span>: <span id="#" class="mention-company" about="Amazon">Amazon</span>. The company had started to market its technology to <span id="#" class="mention-organization" about="LawEnforcement">police departments</span> and <span id="#" class="mention-organization" about="USGovernment">government agencies</span> under the name <span id="#" class="mention-technology" about="AmazonRekognition">Amazon Rekognition</span>.</p>

<p><span id="#" class="mention-person" about="JoyBuolamwini">Ms. Buolamwini</span> and <span id="#" class="mention-person" about="DeborahRaji">Ms. Raji</span> published a study showing that an <span id="#" class="mention-technology" about="FacialRecognition"><span id="#" class="mention-company" about="Amazon">Amazon</span> face service</span> also had <span id="#" class="mention-subject" about="Bias">trouble identifying the <span id="#" class="mention-subject" about="Gender">sex of female</span> and darker-​skinned faces</span>. According to the study, <span id="#" class="mention-technology" about="AmazonRekognition">the service</span> <span id="#" class="mention-subject" about="Bias">mistook <span id="#" class="mention-subject" about="Gender">women</span> for <span id="#" class="mention-subject" about="Gender">men</span> 19 percent of the time and misidentified darker-​skinned <span id="#" class="mention-subject" about="Gender">women</span> for <span id="#" class="mention-subject" about="Gender">men</span> 31 percent of the time. For lighter-​skinned <span id="#" class="mention-subject" about="Gender">males</span>, the error rate was zero</span>.</p>

<p><span id="#" class="mention-company" about="Amazon">Amazon</span> called for <span id="#" class="mention-organization" about="USGovernment">government</span> regulation of <span id="#" class="mention-technology" about="FacialRecognition">facial recognition</span>. It also attacked the researchers in private emails and public blog posts.</p>

<p>“The answer to anxieties over <span id="#" class="mention-technology" about="Technology">new technology</span> is not to run ‘tests’ inconsistent with how the service is designed to be used, and to amplify the test’s false and misleading conclusions through the news media,” an <span id="#" class="mention-company" about="Amazon">Amazon</span> executive, <span id="#" class="mention-person" about="MattWood">Matt Wood</span>, wrote in a blog post that disputed the study and a <span id="#" class="mention-organization" about="TheNewYorkTimes">New York Times</span> article that described it.</p>

<p>In an open letter, <span id="#" class="mention-person" about="MargaretMitchell">Dr. Mitchell</span> and <span id="#" class="mention-person" about="TimnitGebru">Dr. Gebru</span> rejected <span id="#" class="mention-company" about="Amazon">Amazon</span>’s argument and called on it to stop selling to <span id="#" class="mention-organization" about="LawEnforcement">law enforcement</span>. The letter was signed by 25 <span id="#" class="mention-group" about="AICommunity">artificial intelligence researchers</span> from <span id="#" class="mention-company" about="Google">Google</span>, <span id="#" class="mention-company" about="Microsoft">Microsoft</span> and <span id="#" class="mention-organization" about="Academia">academia</span>.</p>

<p>Last June, <span id="#" class="mention-company" about="Amazon">Amazon</span> backed down. It announced that it would not let <span id="#" class="mention-organization" about="LawEnforcement">the police</span> use <span id="#" class="mention-technology" about="AmazonRekognition">its technology</span> for at least a year, saying it wanted to give <span id="#" class="mention-organization" about="USGovernment">Congress</span> time to create rules for the ethical use of the <span id="#" class="mention-technology" about="FacialRecognition">technology</span>. <span id="#" class="mention-organization" about="USGovernment">Congress</span> has yet to take up the issue. <span id="#" class="mention-company" about="Amazon">Amazon</span> declined to comment for this article.</p>

<h2 class="nyt-header end">The End at <span id="#" class="mention-company" about="Google">Google</span></h2>

<p class="after-box"><span id="#" class="mention-person" about="TimnitGebru">Dr. Gebru</span> and <span id="#" class="mention-person" about="MargaretMitchell">Dr. Mitchell</span> had less success fighting for change inside <span id="#" class="mention-company" about="Google">their own company</span>. Corporate gatekeepers at <span id="#" class="mention-company" about="Google">Google</span> were heading them off with a new review system that had lawyers and even communications staff vetting research papers.</p>

<p><span id="#" class="mention-person" about="TimnitGebru">Dr. Gebru</span>’s <span id="#" class="mention-subject" about="Firing">dismissal</span> in December stemmed, she said, from the company’s treatment of a <span id="#" class="mention-subject" about="GebruResearchPaper">research paper</span> she wrote alongside six other researchers, including <span id="#" class="mention-person" about="MargaretMitchell">Dr. Mitchell</span> and three others at <span id="#" class="mention-company" about="Google">Google</span>. <span id="#" class="mention-subject" about="GebruResearchPaper">The paper</span> discussed ways that a <span id="#" class="mention-technology" about="NLP">new type of language technology</span>, including a system built by <span id="#" class="mention-company" about="Google">Google</span> that underpins its <span id="#" class="mention-technology" about="SearchEngines">search engine</span>, can show <span id="#" class="mention-subject" about="Bias">bias against <span id="#" class="mention-subject" about="Gender">women</span> and <span id="#" class="mention-subject" about="Identity">people of color</span></span>.</p>

<p>After she submitted <span id="#" class="mention-subject" about="GebruResearchPaper">the paper</span> to an academic conference, <span id="#" class="mention-person" about="TimnitGebru">Dr. Gebru</span> said, a <span id="#" class="mention-company" about="Google">Google</span> manager demanded that she either retract <span id="#" class="mention-subject" about="GebruResearchPaper">the paper</span> or remove the names of <span id="#" class="mention-company" about="Google">Google</span> employees. She said she would resign if the company could not tell her why it wanted her to retract <span id="#" class="mention-subject" about="GebruResearchPaper">the paper</span> and answer other concerns.</p>

<p>The response: <span id="#" class="mention-subject" about="Firing">Her resignation was accepted immediately</span>, and <span id="#" class="mention-company" about="Google">Google</span> revoked her access to company email and other services. A month later, <span id="#" class="mention-subject" about="Firing">it removed</span> <span id="#" class="mention-person" about="MargaretMitchell">Dr. Mitchell</span>’s access after she searched through her own email in an effort to defend <span id="#" class="mention-person" about="TimnitGebru">Dr. Gebru</span>.</p>

<p>In a <span id="#" class="mention-company" about="Google">Google</span> staff meeting last month, just after the company <span id="#" class="mention-subject" about="Firing">fired</span> <span id="#" class="mention-person" about="MargaretMitchell">Dr. Mitchell</span>, the head of the <span id="#" class="mention-group" about="GoogleAILab">Google A.I. lab</span>, <span id="#" class="mention-person" about="JeffDean">Jeff Dean</span>, said the company would create strict rules meant to limit its review of sensitive research papers. <span id="#" class="mention-person" about="JeffDean">He</span> also defended the reviews. <span id="#" class="mention-person" about="JeffDean">He</span> declined to discuss the details of <span id="#" class="mention-person" about="MargaretMitchell">Dr. Mitchell</span>’s <span id="#" class="mention-subject" about="Firing">dismissal</span> but said she had violated the company’s code of conduct and security policies.</p>

<p>One of <span id="#" class="mention-person" about="JeffDean">Mr. Dean</span>’s new lieutenants, <span id="#" class="mention-person" about="ZoubinGhahramani">Zoubin Ghahramani</span>, said the company must be willing to tackle hard issues. There are “uncomfortable things that <span id="#" class="mention-technology" about="AI">responsible A.I.</span> will inevitably bring up,” <span id="#" class="mention-person" about="ZoubinGhahramani">he</span> said. “We need to be comfortable with that discomfort.”</p>

<p>But it will be difficult for <span id="#" class="mention-company" about="Google">Google</span> to regain trust — both inside the company and out.</p>

<p>“They think they can get away with <span id="#" class="mention-subject" about="Firing">firing these people</span> and it will not hurt them in the end, but they are absolutely shooting themselves in the foot,” said <span id="#" class="mention-person" about="AlexHanna">Alex Hanna</span>, a longtime part of <span id="#" class="mention-company" about="Google">Google</span>’s 10-member <span id="#" class="mention-organization" about="GoogleEthicalAI">Ethical A.I. team</span>. “What they have done is incredibly myopic.”</p>

<p class="update"><span id="#" class="mention-person" about="CadeMetz">Cade Metz</span> is a technology correspondent at <span id="#" class="mention-organization" about="TheNewYorkTimes">The Times</span> and the author of “Genius Makers: The Mavericks Who Brought A.I. to <span id="#" class="mention-company" about="Google">Google</span>, <span id="#" class="mention-company" about="Facebook">Facebook</span>, and the World,” from which this article is adapted. <span class="end-dot"></span></p>
